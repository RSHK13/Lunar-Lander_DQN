{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d006a961bfd4e14b33c2fb75ec48461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af537100479543f988e57429a4c98731",
              "IPY_MODEL_c2acdd51be2847768d7323608e153706",
              "IPY_MODEL_42895d4c73114e0691e1e4a4c8e605fe"
            ],
            "layout": "IPY_MODEL_cde83fe2f1b84e0fb76907ef916c5441"
          }
        },
        "af537100479543f988e57429a4c98731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24aed8923a3b48aeb3739aefb1e54ad3",
            "placeholder": "​",
            "style": "IPY_MODEL_35c2760390a9494eb4e1e468fe863938",
            "value": "100%"
          }
        },
        "c2acdd51be2847768d7323608e153706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ac3ad81f5d42fd94659015cc9c1b08",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_164ff8f44223455a86e96069ccbf00d5",
            "value": 5
          }
        },
        "42895d4c73114e0691e1e4a4c8e605fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_864876f7f37c44a5b4cf79a384eefc9d",
            "placeholder": "​",
            "style": "IPY_MODEL_2f160acf1b2e48069890e012072c8576",
            "value": " 5/5 [00:01&lt;00:00,  3.74it/s]"
          }
        },
        "cde83fe2f1b84e0fb76907ef916c5441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24aed8923a3b48aeb3739aefb1e54ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35c2760390a9494eb4e1e468fe863938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48ac3ad81f5d42fd94659015cc9c1b08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "164ff8f44223455a86e96069ccbf00d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "864876f7f37c44a5b4cf79a384eefc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f160acf1b2e48069890e012072c8576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EZOiodc3R-ke"
      },
      "outputs": [],
      "source": [
        "! pip install -q gym[box2d]==0.25.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from gym import make\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "GAMMA = 0.99\n",
        "TAU = 1e-3\n",
        "INITIAL_STEPS = 1024\n",
        "TRANSITIONS = 500_000\n",
        "STEPS_PER_UPDATE = 4\n",
        "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 5e-4\n",
        "HID_DIM = 64\n",
        "ENV_NAME = \"LunarLander-v2\""
      ],
      "metadata": {
        "id": "gOke8uf8SEyF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42) -> None:\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "def evaluate_policy(agent, episodes=5, verbose=False):\n",
        "    env = make(ENV_NAME)\n",
        "    returns = []\n",
        "    if verbose:\n",
        "        pbar = tqdm(total=episodes)\n",
        "    for _ in range(episodes):\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        total_reward = 0.0\n",
        "\n",
        "        while not done:\n",
        "            state, reward, done, *_ = env.step(agent.act(state))\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "\n",
        "        if verbose:\n",
        "            pbar.update(1)\n",
        "\n",
        "    return returns"
      ],
      "metadata": {
        "id": "hchJpB99SEuo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "    \"Buffer for DeepQNetwork\"\n",
        "\n",
        "    def __init__(self, capacity=10_000, device=DEVICE):\n",
        "        self.capacity = capacity\n",
        "        self.n_stored = 0\n",
        "        self.next_idx = 0\n",
        "        self.device = device\n",
        "\n",
        "        self.state = None\n",
        "        self.action = None\n",
        "        self.next_state = None\n",
        "        self.reward = None\n",
        "        self.done = None\n",
        "\n",
        "    def is_samplable(self, replay_size):\n",
        "        return replay_size <= self.n_stored\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        state: list,\n",
        "        action: int,\n",
        "        next_state: list,\n",
        "        reward: float,\n",
        "        is_done: bool,\n",
        "    ):\n",
        "        state = torch.tensor(state)\n",
        "        next_state = torch.tensor(next_state)\n",
        "\n",
        "        if self.state is None:\n",
        "            self.state = torch.empty(\n",
        "                [self.capacity] + list(state.shape),\n",
        "                dtype=torch.float32,\n",
        "                device=self.device,\n",
        "            )\n",
        "            self.action = torch.empty(\n",
        "                self.capacity, dtype=torch.long, device=self.device\n",
        "            )\n",
        "            self.next_state = torch.empty(\n",
        "                [self.capacity] + list(state.shape),\n",
        "                dtype=torch.float32,\n",
        "                device=self.device,\n",
        "            )\n",
        "            self.reward = torch.empty(\n",
        "                self.capacity, dtype=torch.float32, device=self.device\n",
        "            )\n",
        "            self.done = torch.empty(self.capacity, dtype=torch.long, device=self.device)\n",
        "        self.state[self.next_idx] = state\n",
        "        self.action[self.next_idx] = action\n",
        "        self.next_state[self.next_idx] = next_state\n",
        "        self.reward[self.next_idx] = reward\n",
        "        self.done[self.next_idx] = is_done\n",
        "        self.next_idx = (self.next_idx + 1) % self.capacity\n",
        "        self.n_stored = min(self.capacity, self.n_stored + 1)\n",
        "\n",
        "    def get_batch(self, replay_size=BATCH_SIZE):\n",
        "        idxes = torch.randperm(self.n_stored)[:replay_size]\n",
        "        return (\n",
        "            self.state[idxes],\n",
        "            self.action[idxes].view(-1, 1),\n",
        "            self.next_state[idxes],\n",
        "            self.reward[idxes].view(-1, 1),\n",
        "            self.done[idxes].view(-1, 1),\n",
        "        )"
      ],
      "metadata": {
        "id": "8smmEfGBSErx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQNetworkModel(torch.nn.Module):\n",
        "    \"Classic DQN\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, hid_dim=HID_DIM):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.fc1 = nn.Linear(state_dim, hid_dim)\n",
        "        self.fc2 = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc3 = nn.Linear(hid_dim, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        h = self.activation(self.fc1(state))\n",
        "        h = self.activation(self.fc2(h))\n",
        "        out = self.fc3(h)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rur0OGU6SEpI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN_Agent:\n",
        "    def __init__(self, state_dim, action_dim, hid_dim=64):\n",
        "        self.steps = 0\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self._buffer = ExperienceBuffer(10**5)\n",
        "        self.local_model = DeepQNetworkModel(state_dim, action_dim, hid_dim).to(DEVICE)\n",
        "        self.target_model = DeepQNetworkModel(state_dim, action_dim, hid_dim).to(DEVICE)\n",
        "        self.target_model.eval()\n",
        "        self.optimizer = Adam(self.local_model.parameters())\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def consume_transition(self, transition):\n",
        "        self._buffer.add(*transition)\n",
        "\n",
        "    def sample_batch(self):\n",
        "        return self._buffer.get_batch()\n",
        "    def train_step(self, batch):\n",
        "        # Use batch to update DQN's network.\n",
        "        states, actions, next_states, rewards, dones = batch\n",
        "\n",
        "        q_pred = self.local_model(states).gather(1, actions)\n",
        "        with torch.no_grad():\n",
        "            q_next = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
        "        q_target = rewards + GAMMA * q_next * (1 - dones)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.criterion(q_pred, q_target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self._soft_update_target_network()\n",
        "\n",
        "    def _soft_update_target_network(self):\n",
        "        for target_param, local_param in zip(\n",
        "            self.target_model.parameters(), self.local_model.parameters()\n",
        "        ):\n",
        "            target_param.data.copy_(\n",
        "                TAU * local_param.data + (1.0 - TAU) * target_param.data\n",
        "            )\n",
        "    def update_target_network(self):\n",
        "        self.target_model = copy.deepcopy(self.local_model)\n",
        "\n",
        "    def act(self, state, target=False):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        self.local_model.eval()\n",
        "        with torch.no_grad():\n",
        "            action = np.argmax(self.local_model(state).cpu().numpy())\n",
        "        self.local_model.train()\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.consume_transition(transition)\n",
        "        if self.steps % STEPS_PER_UPDATE == 0:\n",
        "            batch = self.sample_batch()\n",
        "            self.train_step(batch)\n",
        "        if self.steps % STEPS_PER_TARGET_UPDATE == 0:\n",
        "            self.update_target_network()\n",
        "        self.steps += 1\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.local_model.state_dict(), \"agent.pth\")\n"
      ],
      "metadata": {
        "id": "TBg5qkOQSEmQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(SEED)\n",
        "env = make(\"LunarLander-v2\")\n",
        "dqn = DQN_Agent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n, hid_dim=HID_DIM)\n",
        "eps = 0.1\n",
        "state = env.reset()\n",
        "\n",
        "for _ in range(INITIAL_STEPS):\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    next_state, reward, done, *_ = env.step(action)\n",
        "    dqn.consume_transition((state, action, next_state, reward, done))\n",
        "\n",
        "    state = next_state if not done else env.reset()\n",
        "\n",
        "best_avg_rewards = -np.inf\n",
        "# pbar = tqdm(total=TRANSITIONS)\n",
        "for i in range(TRANSITIONS):\n",
        "    # Epsilon-greedy policy\n",
        "    if random.random() < eps:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = dqn.act(state)\n",
        "    next_state, reward, done, *_ = env.step(action)\n",
        "    dqn.update((state, action, next_state, reward, done))\n",
        "\n",
        "    state = next_state if not done else env.reset()\n",
        "\n",
        "#     pbar.update(1)\n",
        "\n",
        "    if (i + 1) % (TRANSITIONS // 100) == 0:\n",
        "        rewards = evaluate_policy(dqn, 5)\n",
        "        avg_reward = np.mean(rewards)\n",
        "#         pbar.set_description(\n",
        "#             f\"Best reward mean: {best_avg_rewards:.2f}, Reward mean: {avg_reward:.2f}, Reward std: {np.std(rewards):.2f}\"\n",
        "#         )\n",
        "        print(f\"Step: {i + 1}/{TRANSITIONS}, Best reward mean: {best_avg_rewards:.2f}, Reward mean: {avg_reward:.2f}, Reward std: {np.std(rewards):.2f}\")\n",
        "        if avg_reward > best_avg_rewards:\n",
        "            best_avg_rewards = avg_reward\n",
        "            dqn.save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zpxgm8AkSEjX",
        "outputId": "3c5d9890-f51b-49ea-a033-6f1e31a42bed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 5000/500000, Best reward mean: -inf, Reward mean: -148.01, Reward std: 91.33\n",
            "Step: 10000/500000, Best reward mean: -148.01, Reward mean: -381.77, Reward std: 119.30\n",
            "Step: 15000/500000, Best reward mean: -148.01, Reward mean: -127.34, Reward std: 18.98\n",
            "Step: 20000/500000, Best reward mean: -127.34, Reward mean: -92.44, Reward std: 21.29\n",
            "Step: 25000/500000, Best reward mean: -92.44, Reward mean: -111.07, Reward std: 44.43\n",
            "Step: 30000/500000, Best reward mean: -92.44, Reward mean: -134.66, Reward std: 21.87\n",
            "Step: 35000/500000, Best reward mean: -92.44, Reward mean: -170.39, Reward std: 62.93\n",
            "Step: 40000/500000, Best reward mean: -92.44, Reward mean: -158.75, Reward std: 86.34\n",
            "Step: 45000/500000, Best reward mean: -92.44, Reward mean: -181.20, Reward std: 83.51\n",
            "Step: 50000/500000, Best reward mean: -92.44, Reward mean: -165.73, Reward std: 98.22\n",
            "Step: 55000/500000, Best reward mean: -92.44, Reward mean: -121.97, Reward std: 48.96\n",
            "Step: 60000/500000, Best reward mean: -92.44, Reward mean: -70.44, Reward std: 99.83\n",
            "Step: 65000/500000, Best reward mean: -70.44, Reward mean: -217.20, Reward std: 34.10\n",
            "Step: 70000/500000, Best reward mean: -70.44, Reward mean: -90.60, Reward std: 54.92\n",
            "Step: 75000/500000, Best reward mean: -70.44, Reward mean: -41.16, Reward std: 10.52\n",
            "Step: 80000/500000, Best reward mean: -41.16, Reward mean: -121.72, Reward std: 103.47\n",
            "Step: 85000/500000, Best reward mean: -41.16, Reward mean: -33.44, Reward std: 19.67\n",
            "Step: 90000/500000, Best reward mean: -33.44, Reward mean: -73.45, Reward std: 15.48\n",
            "Step: 95000/500000, Best reward mean: -33.44, Reward mean: -61.53, Reward std: 35.57\n",
            "Step: 100000/500000, Best reward mean: -33.44, Reward mean: -70.60, Reward std: 27.82\n",
            "Step: 105000/500000, Best reward mean: -33.44, Reward mean: -111.36, Reward std: 25.31\n",
            "Step: 110000/500000, Best reward mean: -33.44, Reward mean: -67.73, Reward std: 29.10\n",
            "Step: 115000/500000, Best reward mean: -33.44, Reward mean: -142.56, Reward std: 35.29\n",
            "Step: 120000/500000, Best reward mean: -33.44, Reward mean: -67.19, Reward std: 18.15\n",
            "Step: 125000/500000, Best reward mean: -33.44, Reward mean: -28.28, Reward std: 13.91\n",
            "Step: 130000/500000, Best reward mean: -28.28, Reward mean: -88.62, Reward std: 68.09\n",
            "Step: 135000/500000, Best reward mean: -28.28, Reward mean: -39.90, Reward std: 21.80\n",
            "Step: 140000/500000, Best reward mean: -28.28, Reward mean: -63.16, Reward std: 49.87\n",
            "Step: 145000/500000, Best reward mean: -28.28, Reward mean: -56.55, Reward std: 13.47\n",
            "Step: 150000/500000, Best reward mean: -28.28, Reward mean: -64.75, Reward std: 15.31\n",
            "Step: 155000/500000, Best reward mean: -28.28, Reward mean: -34.95, Reward std: 15.86\n",
            "Step: 160000/500000, Best reward mean: -28.28, Reward mean: -63.39, Reward std: 11.60\n",
            "Step: 165000/500000, Best reward mean: -28.28, Reward mean: -55.13, Reward std: 27.32\n",
            "Step: 170000/500000, Best reward mean: -28.28, Reward mean: -56.77, Reward std: 19.90\n",
            "Step: 175000/500000, Best reward mean: -28.28, Reward mean: -45.56, Reward std: 19.07\n",
            "Step: 180000/500000, Best reward mean: -28.28, Reward mean: -25.27, Reward std: 18.53\n",
            "Step: 185000/500000, Best reward mean: -25.27, Reward mean: -75.81, Reward std: 34.95\n",
            "Step: 190000/500000, Best reward mean: -25.27, Reward mean: -27.02, Reward std: 23.61\n",
            "Step: 195000/500000, Best reward mean: -25.27, Reward mean: -73.70, Reward std: 40.64\n",
            "Step: 200000/500000, Best reward mean: -25.27, Reward mean: -38.74, Reward std: 19.65\n",
            "Step: 205000/500000, Best reward mean: -25.27, Reward mean: -53.08, Reward std: 16.99\n",
            "Step: 210000/500000, Best reward mean: -25.27, Reward mean: -65.67, Reward std: 20.89\n",
            "Step: 215000/500000, Best reward mean: -25.27, Reward mean: -51.19, Reward std: 17.07\n",
            "Step: 220000/500000, Best reward mean: -25.27, Reward mean: -34.57, Reward std: 16.95\n",
            "Step: 225000/500000, Best reward mean: -25.27, Reward mean: -33.56, Reward std: 50.80\n",
            "Step: 230000/500000, Best reward mean: -25.27, Reward mean: -36.64, Reward std: 19.99\n",
            "Step: 235000/500000, Best reward mean: -25.27, Reward mean: -23.06, Reward std: 5.02\n",
            "Step: 240000/500000, Best reward mean: -23.06, Reward mean: -33.15, Reward std: 20.30\n",
            "Step: 245000/500000, Best reward mean: -23.06, Reward mean: -22.70, Reward std: 8.59\n",
            "Step: 250000/500000, Best reward mean: -22.70, Reward mean: -111.46, Reward std: 100.03\n",
            "Step: 255000/500000, Best reward mean: -22.70, Reward mean: -52.14, Reward std: 13.08\n",
            "Step: 260000/500000, Best reward mean: -22.70, Reward mean: -48.17, Reward std: 17.02\n",
            "Step: 265000/500000, Best reward mean: -22.70, Reward mean: -16.18, Reward std: 11.25\n",
            "Step: 270000/500000, Best reward mean: -16.18, Reward mean: -49.77, Reward std: 16.31\n",
            "Step: 275000/500000, Best reward mean: -16.18, Reward mean: -79.72, Reward std: 10.11\n",
            "Step: 280000/500000, Best reward mean: -16.18, Reward mean: -37.84, Reward std: 11.79\n",
            "Step: 285000/500000, Best reward mean: -16.18, Reward mean: -25.16, Reward std: 16.40\n",
            "Step: 290000/500000, Best reward mean: -16.18, Reward mean: -22.28, Reward std: 22.68\n",
            "Step: 295000/500000, Best reward mean: -16.18, Reward mean: -43.82, Reward std: 18.25\n",
            "Step: 300000/500000, Best reward mean: -16.18, Reward mean: -24.65, Reward std: 9.95\n",
            "Step: 305000/500000, Best reward mean: -16.18, Reward mean: -36.76, Reward std: 18.70\n",
            "Step: 310000/500000, Best reward mean: -16.18, Reward mean: -34.96, Reward std: 14.43\n",
            "Step: 315000/500000, Best reward mean: -16.18, Reward mean: -38.39, Reward std: 28.70\n",
            "Step: 320000/500000, Best reward mean: -16.18, Reward mean: -25.28, Reward std: 30.54\n",
            "Step: 325000/500000, Best reward mean: -16.18, Reward mean: -0.91, Reward std: 23.41\n",
            "Step: 330000/500000, Best reward mean: -0.91, Reward mean: 48.77, Reward std: 93.38\n",
            "Step: 335000/500000, Best reward mean: 48.77, Reward mean: 228.56, Reward std: 19.59\n",
            "Step: 340000/500000, Best reward mean: 228.56, Reward mean: 163.24, Reward std: 42.02\n",
            "Step: 345000/500000, Best reward mean: 228.56, Reward mean: 24.81, Reward std: 272.60\n",
            "Step: 350000/500000, Best reward mean: 228.56, Reward mean: -4.90, Reward std: 142.78\n",
            "Step: 355000/500000, Best reward mean: 228.56, Reward mean: 191.12, Reward std: 33.41\n",
            "Step: 360000/500000, Best reward mean: 228.56, Reward mean: 172.82, Reward std: 85.12\n",
            "Step: 365000/500000, Best reward mean: 228.56, Reward mean: 181.86, Reward std: 29.41\n",
            "Step: 370000/500000, Best reward mean: 228.56, Reward mean: 150.16, Reward std: 74.36\n",
            "Step: 375000/500000, Best reward mean: 228.56, Reward mean: 141.43, Reward std: 125.86\n",
            "Step: 380000/500000, Best reward mean: 228.56, Reward mean: 154.83, Reward std: 59.38\n",
            "Step: 385000/500000, Best reward mean: 228.56, Reward mean: 217.66, Reward std: 55.74\n",
            "Step: 390000/500000, Best reward mean: 228.56, Reward mean: -5.64, Reward std: 116.29\n",
            "Step: 395000/500000, Best reward mean: 228.56, Reward mean: 109.70, Reward std: 82.08\n",
            "Step: 400000/500000, Best reward mean: 228.56, Reward mean: 134.59, Reward std: 25.33\n",
            "Step: 405000/500000, Best reward mean: 228.56, Reward mean: 132.25, Reward std: 119.01\n",
            "Step: 410000/500000, Best reward mean: 228.56, Reward mean: -6.77, Reward std: 161.44\n",
            "Step: 415000/500000, Best reward mean: 228.56, Reward mean: 130.92, Reward std: 50.49\n",
            "Step: 420000/500000, Best reward mean: 228.56, Reward mean: 221.05, Reward std: 66.77\n",
            "Step: 425000/500000, Best reward mean: 228.56, Reward mean: 209.27, Reward std: 75.00\n",
            "Step: 430000/500000, Best reward mean: 228.56, Reward mean: 183.10, Reward std: 43.32\n",
            "Step: 435000/500000, Best reward mean: 228.56, Reward mean: 183.78, Reward std: 59.32\n",
            "Step: 440000/500000, Best reward mean: 228.56, Reward mean: 219.17, Reward std: 49.22\n",
            "Step: 445000/500000, Best reward mean: 228.56, Reward mean: 244.39, Reward std: 9.14\n",
            "Step: 450000/500000, Best reward mean: 244.39, Reward mean: 201.16, Reward std: 41.72\n",
            "Step: 455000/500000, Best reward mean: 244.39, Reward mean: 204.83, Reward std: 77.59\n",
            "Step: 460000/500000, Best reward mean: 244.39, Reward mean: 213.43, Reward std: 96.07\n",
            "Step: 465000/500000, Best reward mean: 244.39, Reward mean: 212.29, Reward std: 24.69\n",
            "Step: 470000/500000, Best reward mean: 244.39, Reward mean: 248.43, Reward std: 24.21\n",
            "Step: 475000/500000, Best reward mean: 248.43, Reward mean: 238.55, Reward std: 21.49\n",
            "Step: 480000/500000, Best reward mean: 248.43, Reward mean: 261.06, Reward std: 24.42\n",
            "Step: 485000/500000, Best reward mean: 261.06, Reward mean: 206.25, Reward std: 80.23\n",
            "Step: 490000/500000, Best reward mean: 261.06, Reward mean: 251.85, Reward std: 20.73\n",
            "Step: 495000/500000, Best reward mean: 261.06, Reward mean: 246.60, Reward std: 10.68\n",
            "Step: 500000/500000, Best reward mean: 261.06, Reward mean: 237.10, Reward std: 51.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "xYFhl8tkVDP9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, weights=\"agent.pth\"):\n",
        "        self.model = DeepQNetworkModel(8, 4, 64)\n",
        "        weights = torch.load(weights, map_location=DEVICE)\n",
        "        self.model.load_state_dict(weights)\n",
        "        self.model.to(DEVICE)\n",
        "        self.model.eval()\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            action = np.argmax(self.model(state).cpu().numpy())\n",
        "        return action"
      ],
      "metadata": {
        "id": "QwCEmc0KVDMi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AGENT_WEIGHTS_PATH = \"agent.pth\"\n",
        "AGENT_WEIGHTS_PATH = \"agent.pth\"\n",
        "agent = Agent(AGENT_WEIGHTS_PATH)\n",
        "rewards = evaluate_policy(agent, 5, True)\n",
        "print(\"Averege reward on 5 episodes:\", np.mean(rewards))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "8d006a961bfd4e14b33c2fb75ec48461",
            "af537100479543f988e57429a4c98731",
            "c2acdd51be2847768d7323608e153706",
            "42895d4c73114e0691e1e4a4c8e605fe",
            "cde83fe2f1b84e0fb76907ef916c5441",
            "24aed8923a3b48aeb3739aefb1e54ad3",
            "35c2760390a9494eb4e1e468fe863938",
            "48ac3ad81f5d42fd94659015cc9c1b08",
            "164ff8f44223455a86e96069ccbf00d5",
            "864876f7f37c44a5b4cf79a384eefc9d",
            "2f160acf1b2e48069890e012072c8576"
          ]
        },
        "id": "0iRaSZqZVDJq",
        "outputId": "b691d62d-eb22-4796-8cbb-e94794307570"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d006a961bfd4e14b33c2fb75ec48461"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averege reward on 5 episodes: 233.74213239901565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython import display\n",
        "\n",
        "def show_video(env_name, video_dir=\".\"):\n",
        "    mp4list = glob.glob(f'{video_dir}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = f'{video_dir}/{env_name}.mp4'\n",
        "        video = io.open(mp4, 'rb').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(display.HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def render_video_of_model(agent, env_name):\n",
        "    env = make(env_name)\n",
        "    vid = video_recorder.VideoRecorder(env, path=f\"{env_name}.mp4\")\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        frame = env.render(mode='rgb_array')\n",
        "        vid.capture_frame()\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "    env.close()\n",
        "\n",
        "\n",
        "# Not work in kaggle!\n",
        "# render_video_of_model(agent, ENV_NAME)\n",
        "# show_video(ENV_NAME)\n",
        "\n",
        "show_video(ENV_NAME, video_dir=\"rl-agents\")"
      ],
      "metadata": {
        "id": "_s8ebnKsVDD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IxuhfTwVDBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jf2vJhsISEgk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}